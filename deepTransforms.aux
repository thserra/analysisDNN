\relax 
\catcode `"\active 
\catcode `-\active 
\citation{Hornik1989,Cybenko1989}
\citation{Delalleau2011}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {title}{Transforming Deep Neural Networks}{1}}
\@writefile{toc}{\authcount {1}}
\@writefile{toc}{\contentsline {author}{Srikumar Ramalingam}{1}}
\newlabel{def.equivalent_networks}{{1}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Reduction of deep neural networks}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces We show a standard network {\sc  DNN1} with $n$ input variables, $L$ hidden layers, and one output variable.}}{1}}
\newlabel{fg.DNN1}{{1}{1}}
\@writefile{toc}{\contentsline {paragraph}{{\sc  DNN1:}}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces (a) We show a simple network with 3 hidden layers having two neurons each. (b) Given an input $\mathchoice {\unhbox \voidb@x \hbox {\relax \mathversion  {bold}$\displaystyle x$}} {\unhbox \voidb@x \hbox {\relax \mathversion  {bold}$\textstyle x$}} {\unhbox \voidb@x \hbox {\relax \mathversion  {bold}$\scriptstyle x$}} {\unhbox \voidb@x \hbox {\relax \mathversion  {bold}$\scriptscriptstyle x$}}$ we show the outputs from each hidden layer in the form of a decision tree. The leaves in the decision tree correspond to all possible outputs $y$ that can be obtained from the network shown in (a).}}{2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{2}}
\newlabel{fg.3_layer_eg}{{2}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces {\sc  DNN2} of Figure~\ref  {fg.3_layer_eg}(a):}}{2}}
\newlabel{fg.3_layer_eg_DNN2}{{3}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Activation Set}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Decision tree interpretation}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Reduction of networks}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces {\sc  DNN2:} Given the {\sc  DNN1} shown in Figure~\ref  {fg.DNN1}, we show an equivalent network {\sc  DNN2} with only 2 hidden layers.}}{4}}
\newlabel{fg.DNN2}{{4}{4}}
\newlabel{eq.deepTransformL_To_2}{{1}{4}}
\newlabel{eq.deepTransformL_To_L-1}{{2}{4}}
\citation{Zaslavsky1975}
\citation{Zaslavsky1975}
\bibstyle{plain}
\bibdata{dam}
\bibcite{Cybenko1989}{1}
\bibcite{Delalleau2011}{2}
\bibcite{Hornik1989}{3}
\bibcite{Zaslavsky1975}{4}
\@writefile{toc}{\contentsline {section}{\numberline {3}Hyperplane arrangments}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Special Cases}{5}}
\newlabel{eq.deepTransformL_To_0_positiveweights}{{3}{5}}
\newlabel{eq.eq.deepTransformL_To_0_negativeweights}{{4}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Decompressing Networks}{5}}
